{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62aca66-97f7-4ee6-9d59-13cce0d0b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   #work with folders & files\n",
    "import re   #text pattern matching\n",
    "import io   #handle image/text streams\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract   #Tesseract reads text from images\n",
    "\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec   #stores embeddings (vector database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8036ea28-3253-4f75-b581-292638270292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tesseract path set: C:\\tesseract\\tesseract.exe\n"
     ]
    }
   ],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd =r'C:\\tesseract\\tesseract.exe'\n",
    "print(\"âœ… Tesseract path set:\", pytesseract.pytesseract.tesseract_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27f3ff-e233-4c74-aaea-b2b249a96c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clients ready\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = \"Paste Your OpenAI API Key\"\n",
    "PINECONE_API_KEY = \"Paste Your Pinecone API Key\" \n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "print(\"âœ… Clients ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052c1bf-eb89-428b-a4e6-92d261a15a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pinecone index ready: fda483-rag-v1-clean\n"
     ]
    }
   ],
   "source": [
    "index_name = \"fda483-rag-v1-clean\" # Pinecone Index Name\n",
    "\n",
    "# delete if exists (optional)\n",
    "\"\"\"existing = [i[\"name\"] for i in pc.list_indexes()]\n",
    "if index_name in existing:\n",
    "    pc.delete_index(index_name)\n",
    "    print(\"ðŸ—‘ï¸ Deleted old index\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=1536,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    ")\"\"\"\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(\"âœ… Pinecone index ready:\", index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a41bb5-96d3-4d37-af79-f25c49544e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Converts into numbers (vector) - Embedding\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    res = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return res.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee66b290-0e22-4899-818e-17c13a25e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes too many line breaks and extra spaces\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb52dd00-6760-415e-9aed-b9b6f35f8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF â†’ image â†’ OCR â†’ text\n",
    "\n",
    "def ocr_page(pdf_path, page_number, dpi=300):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(page_number)\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    img_bytes = pix.tobytes(\"png\")\n",
    "    doc.close()\n",
    "\n",
    "    img = Image.open(io.BytesIO(img_bytes))\n",
    "    text = pytesseract.image_to_string(img)\n",
    "\n",
    "    return clean_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079c1efd-60e0-4cb9-89b4-7519f25d7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract full page text and if blank than use OCR\n",
    "\n",
    "\n",
    "def extract_full_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    parts = []\n",
    "\n",
    "    for p in range(len(doc)):\n",
    "        page = doc.load_page(p)\n",
    "        text = clean_text(page.get_text(\"text\"))\n",
    "\n",
    "        # if scanned or empty text -> OCR\n",
    "        if len(text) < 40:\n",
    "            print(f\"âš ï¸ OCR: {os.path.basename(pdf_path)} page {p+1}\")\n",
    "            text = ocr_page(pdf_path, p)\n",
    "\n",
    "        if len(text) >= 40:\n",
    "            parts.append(f\"\\n\\n--- PAGE {p+1} ---\\n\\n{text}\")\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25c6725-e49c-4d1d-86b0-cbca13cdc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses to extract company name and remove dates and numbers from the pdf name\n",
    "\n",
    "def extract_firm_name_from_filename(filename: str):\n",
    "    name = filename.lower().replace(\".pdf\", \"\")\n",
    "    name = name.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "    # remove date patterns + numbers\n",
    "    name = re.sub(r\"\\d{1,2}\\.\\d{1,2}\\-\\d{1,2}\\.\\d{1,2}\\.\\d{2}\", \" \", name)\n",
    "    name = re.sub(r\"\\d+\", \" \", name)\n",
    "\n",
    "    # remove common words\n",
    "    for w in [\"redacted\", \"final\", \"sb\", \"483\"]:\n",
    "        name = name.replace(w, \" \")\n",
    "\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "\n",
    "    # take first 6 words\n",
    "    return \" \".join(name.split()[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e775b923-c53e-451a-8b4c-a378aaa9e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits Observations\n",
    "\n",
    "def split_fda_sections(full_text: str):\n",
    "    full_text = clean_text(full_text)\n",
    "    pattern = r\"\\bOBSERVATION\\s+\\d+\\b\"\n",
    "    matches = list(re.finditer(pattern, full_text, flags=re.IGNORECASE))\n",
    "\n",
    "    if not matches:\n",
    "        return [(\"DOCUMENT\", full_text)]\n",
    "\n",
    "    sections = []\n",
    "    header = full_text[:matches[0].start()].strip()\n",
    "    if header:\n",
    "        sections.append((\"DOCUMENT\", header))\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].start()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(full_text)\n",
    "        title = matches[i].group().upper()\n",
    "        body = full_text[start:end].strip()\n",
    "        sections.append((title, body))\n",
    "\n",
    "    return sections\n",
    "\n",
    "def detect_observation_number(section_title: str):\n",
    "    m = re.search(r\"OBSERVATION\\s+(\\d+)\", section_title, re.IGNORECASE)\n",
    "    return int(m.group(1)) if m else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a0f43d-7f11-4527-80e0-b617def9b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks Creation\n",
    "\n",
    "def make_chunks(text, chunk_size=1200, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    step = chunk_size - overlap\n",
    "\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        ch = text[start:end].strip()\n",
    "        if ch:\n",
    "            chunks.append(ch)\n",
    "        start += step\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d613613-04db-41d9-9c7a-feb5941b1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final chunk creation sections combines observation sections, firm name, source file\n",
    "\n",
    "def build_fda_chunks(full_text, source):\n",
    "    sections = split_fda_sections(full_text)\n",
    "    firm_name = extract_firm_name_from_filename(source)\n",
    "\n",
    "    final_chunks = []\n",
    "    chunk_id = 0\n",
    "\n",
    "    for title, section_text in sections:\n",
    "        obs_num = detect_observation_number(title)\n",
    "        section_chunks = make_chunks(section_text, chunk_size=1200, overlap=200)\n",
    "\n",
    "        for part_no, ch in enumerate(section_chunks, start=1):\n",
    "            chunk_id += 1\n",
    "\n",
    "            metadata = {\n",
    "                \"text\": ch,\n",
    "                \"source\": source,\n",
    "                \"firm_name\": firm_name,\n",
    "                \"section\": title,\n",
    "                \"section_type\": \"observation\" if obs_num is not None else \"header\"\n",
    "            }\n",
    "\n",
    "            if obs_num is not None:\n",
    "                metadata[\"observation_number\"] = str(obs_num)\n",
    "\n",
    "            final_chunks.append({\n",
    "                \"id\": f\"{source}_{title}_part{part_no}_{chunk_id}\",\n",
    "                \"values\": get_embedding(ch),\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "\n",
    "    return final_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34ccc06-d2f9-4ffb-8cb2-c0d9938e63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload chunks to pinecone\n",
    "\n",
    "def upsert_chunks_to_pinecone(vectors, batch_size=50):\n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "        time.sleep(0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f64619-da6e-401d-9b8f-5bec0155f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a time loop while reading the file and extracting chunks\n",
    "\n",
    "def log_progress(i, total, filename, chunk_count, start_time):\n",
    "    elapsed = time.time() - start_time\n",
    "    now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{now}] âœ… PDF {i}/{total} | file={filename} | chunks={chunk_count} | time={elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9b677-aee2-4c56-accf-b778996a42e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDFs found: 56\n",
      "\n",
      "ðŸ“„ Processing alvotech_hf_rekjavik_iceland_6.26-7.4.25_483_redacted.pdf\n",
      "[11:06:22] âœ… PDF 1/56 | file=alvotech_hf_rekjavik_iceland_6.26-7.4.25_483_redacted.pdf | chunks=51 | time=75.8s\n",
      "\n",
      "ðŸ“„ Processing applied_483_682998_redacted.508.pdf\n",
      "[11:06:30] âœ… PDF 2/56 | file=applied_483_682998_redacted.508.pdf | chunks=12 | time=7.5s\n",
      "\n",
      "ðŸ“„ Processing applied_483_691575_redacted.508.pdf\n",
      "[11:07:21] âœ… PDF 3/56 | file=applied_483_691575_redacted.508.pdf | chunks=41 | time=50.6s\n",
      "\n",
      "ðŸ“„ Processing applied_anazaohealth_corporation_3011152407_483_08082025_sb_redacted.508.pdf\n",
      "[11:07:29] âœ… PDF 4/56 | file=applied_anazaohealth_corporation_3011152407_483_08082025_sb_redacted.508.pdf | chunks=15 | time=8.2s\n",
      "\n",
      "ðŸ“„ Processing applied_bso_llc_3011976853_483_07162025_kum_redacted.508.pdf\n",
      "[11:07:47] âœ… PDF 5/56 | file=applied_bso_llc_3011976853_483_07162025_kum_redacted.508.pdf | chunks=36 | time=18.6s\n",
      "\n",
      "ðŸ“„ Processing applied_cw_osrx_inc_3014549846_483_111425_redacted.508.pdf\n",
      "[11:07:58] âœ… PDF 6/56 | file=applied_cw_osrx_inc_3014549846_483_111425_redacted.508.pdf | chunks=17 | time=11.1s\n",
      "\n",
      "ðŸ“„ Processing applied_delta_pharma_ms_3004034796_483_11212025_redacted.508 (1).pdf\n",
      "[11:08:02] âœ… PDF 7/56 | file=applied_delta_pharma_ms_3004034796_483_11212025_redacted.508 (1).pdf | chunks=6 | time=4.0s\n",
      "\n",
      "ðŸ“„ Processing applied_delta_pharma_ms_3004034796_483_11212025_redacted.508.pdf\n",
      "[11:08:31] âœ… PDF 8/56 | file=applied_delta_pharma_ms_3004034796_483_11212025_redacted.508.pdf | chunks=6 | time=28.9s\n",
      "\n",
      "ðŸ“„ Processing applied_empower_pharmacy_3021758709_fda483_11-14-2025_redacted.508 (1).pdf\n",
      "[11:08:46] âœ… PDF 9/56 | file=applied_empower_pharmacy_3021758709_fda483_11-14-2025_redacted.508 (1).pdf | chunks=16 | time=14.6s\n",
      "\n",
      "ðŸ“„ Processing applied_empower_pharmacy_3021758709_fda483_11-14-2025_redacted.508.pdf\n",
      "[11:09:04] âœ… PDF 10/56 | file=applied_empower_pharmacy_3021758709_fda483_11-14-2025_redacted.508.pdf | chunks=16 | time=17.7s\n",
      "\n",
      "ðŸ“„ Processing applied_integradose_compounding_services_llc_3014483112_cp483_7.15-25.25_ct_redacted.508.pdf\n",
      "[11:09:14] âœ… PDF 11/56 | file=applied_integradose_compounding_services_llc_3014483112_cp483_7.15-25.25_ct_redacted.508.pdf | chunks=19 | time=10.6s\n",
      "\n",
      "ðŸ“„ Processing applied_nephron_sc._inc_fei_3010892830_fda_483_8-22-25_redacted.508.pdf\n",
      "[11:09:39] âœ… PDF 12/56 | file=applied_nephron_sc._inc_fei_3010892830_fda_483_8-22-25_redacted.508.pdf | chunks=44 | time=24.5s\n",
      "\n",
      "ðŸ“„ Processing applied_olympia_pharmaceuticals_3009724085_483_0808202_kum_redacted.508.pdf\n",
      "[11:09:46] âœ… PDF 13/56 | file=applied_olympia_pharmaceuticals_3009724085_483_0808202_kum_redacted.508.pdf | chunks=12 | time=7.4s\n",
      "\n",
      "ðŸ“„ Processing applied_park_avenue_compounding_1972829_483_09232025_cm_redacted508.pdf\n",
      "[11:10:55] âœ… PDF 14/56 | file=applied_park_avenue_compounding_1972829_483_09232025_cm_redacted508.pdf | chunks=37 | time=68.8s\n",
      "\n",
      "ðŸ“„ Processing applied_right_value_drug_stores_llc_dba_carie_boyd_pharmaceuticals_fei_3010589333_fda483_7-18-25_mb.508.pdf\n",
      "[11:11:16] âœ… PDF 15/56 | file=applied_right_value_drug_stores_llc_dba_carie_boyd_pharmaceuticals_fei_3010589333_fda483_7-18-25_mb.508.pdf | chunks=22 | time=20.7s\n",
      "\n",
      "ðŸ“„ Processing applied_sca_pharma_3010683157_fda483_10-30-2025_redacted.508.pdf\n",
      "[11:12:15] âœ… PDF 16/56 | file=applied_sca_pharma_3010683157_fda483_10-30-2025_redacted.508.pdf | chunks=13 | time=59.7s\n",
      "\n",
      "ðŸ“„ Processing applied_tailstormhealth_inc_dba_medivanthealth_3022483154_483_111425_redacted.508.pdf\n",
      "[11:12:24] âœ… PDF 17/56 | file=applied_tailstormhealth_inc_dba_medivanthealth_3022483154_483_111425_redacted.508.pdf | chunks=8 | time=8.4s\n",
      "\n",
      "ðŸ“„ Processing aurobindo_pharma_hyderabad_india_8.25-9.5.25_483_redacted.pdf\n",
      "[11:15:01] âœ… PDF 18/56 | file=aurobindo_pharma_hyderabad_india_8.25-9.5.25_483_redacted.pdf | chunks=40 | time=157.6s\n",
      "\n",
      "ðŸ“„ Processing biocon_biologics_bengaluru_india_8.26-9.3.25_483_redacted.pdf\n",
      "[11:16:07] âœ… PDF 19/56 | file=biocon_biologics_bengaluru_india_8.26-9.3.25_483_redacted.pdf | chunks=27 | time=66.1s\n",
      "\n",
      "ðŸ“„ Processing bso_golden_co_final_483_06_27_2025_redacted.508.pdf\n",
      "[11:18:18] âœ… PDF 20/56 | file=bso_golden_co_final_483_06_27_2025_redacted.508.pdf | chunks=35 | time=130.9s\n",
      "\n",
      "ðŸ“„ Processing caps_allentown_300950582_483_090525_redacted.508.pdf\n",
      "[11:19:40] âœ… PDF 21/56 | file=caps_allentown_300950582_483_090525_redacted.508.pdf | chunks=19 | time=81.4s\n",
      "\n",
      "ðŸ“„ Processing catalent_indiana_llc_fda_483_redacted_1.508.pdf\n",
      "[11:21:11] âœ… PDF 22/56 | file=catalent_indiana_llc_fda_483_redacted_1.508.pdf | chunks=47 | time=90.8s\n",
      "\n",
      "ðŸ“„ Processing cohance_lifesciences_ltd._telangana_india_8.4-8.12.25_483_redacted.pdf\n",
      "[11:22:15] âœ… PDF 23/56 | file=cohance_lifesciences_ltd._telangana_india_8.4-8.12.25_483_redacted.pdf | chunks=41 | time=64.8s\n",
      "\n",
      "ðŸ“„ Processing dr._reddys_laboratories_andhra_pradesh_india_7.10-7.18.25_483_redacted_0.pdf\n",
      "[11:22:28] âœ… PDF 24/56 | file=dr._reddys_laboratories_andhra_pradesh_india_7.10-7.18.25_483_redacted_0.pdf | chunks=20 | time=13.0s\n",
      "\n",
      "ðŸ“„ Processing dr._reddys_laboratories_limited_unit_6_srikakulam_india_6.7.24_483.pdf\n",
      "[11:23:21] âœ… PDF 25/56 | file=dr._reddys_laboratories_limited_unit_6_srikakulam_india_6.7.24_483.pdf | chunks=23 | time=53.1s\n",
      "\n",
      "ðŸ“„ Processing dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 1\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 2\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 3\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 4\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 5\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 6\n",
      "âš ï¸ OCR: dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf page 7\n",
      "[11:24:41] âœ… PDF 26/56 | file=dr._reddys_laboratories_mirfield_west_yorkshire_uk_9.1-9.5.25_483_redacted.pdf | chunks=21 | time=79.9s\n",
      "\n",
      "ðŸ“„ Processing dr._reddys_telangana_india_9.4-9.12.25_483_redacted.pdf\n",
      "[11:26:59] âœ… PDF 27/56 | file=dr._reddys_telangana_india_9.4-9.12.25_483_redacted.pdf | chunks=24 | time=138.1s\n",
      "\n",
      "ðŸ“„ Processing fagron_compounding_services_483_redacted.508.pdf\n",
      "[11:28:45] âœ… PDF 28/56 | file=fagron_compounding_services_483_redacted.508.pdf | chunks=32 | time=105.2s\n",
      "\n",
      "ðŸ“„ Processing fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 1\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 2\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 3\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 4\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 5\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 6\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 7\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 8\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 9\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 10\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 11\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 12\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 13\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 14\n",
      "âš ï¸ OCR: fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf page 15\n",
      "[11:32:09] âœ… PDF 29/56 | file=fareva_amboise_poce_sur_cisse_france_9.8-9.16.25_483_redacted.pdf | chunks=38 | time=204.6s\n",
      "\n",
      "ðŸ“„ Processing farmakeio_outsourcing_llc_3014982757_cp483_8.18.25_-_9.05.25_redacted.508.pdf\n",
      "[11:33:36] âœ… PDF 30/56 | file=farmakeio_outsourcing_llc_3014982757_cp483_8.18.25_-_9.05.25_redacted.508.pdf | chunks=19 | time=87.1s\n",
      "\n",
      "ðŸ“„ Processing genogenix_llc_fei_3032144832_fda_483_7-18-25_mb_redacted.508.pdf\n",
      "[11:34:08] âœ… PDF 31/56 | file=genogenix_llc_fei_3032144832_fda_483_7-18-25_mb_redacted.508.pdf | chunks=55 | time=32.1s\n",
      "\n",
      "ðŸ“„ Processing immacule_lifesciences_9.8-9.18.25_483_redacted.pdf\n",
      "[11:34:50] âœ… PDF 32/56 | file=immacule_lifesciences_9.8-9.18.25_483_redacted.pdf | chunks=49 | time=41.3s\n",
      "\n",
      "ðŸ“„ Processing inotiv_inc._west_lafayette-in_10-20_through_10-25-2025_fda_483_redacted_0.pdf\n",
      "âš ï¸ OCR: inotiv_inc._west_lafayette-in_10-20_through_10-25-2025_fda_483_redacted_0.pdf page 1\n",
      "[11:35:41] âœ… PDF 33/56 | file=inotiv_inc._west_lafayette-in_10-20_through_10-25-2025_fda_483_redacted_0.pdf | chunks=3 | time=51.6s\n",
      "\n",
      "ðŸ“„ Processing lonza_guangzhou_guangdong_china_10.27-10.31.25_483_redacted.pdf\n",
      "âš ï¸ OCR: lonza_guangzhou_guangdong_china_10.27-10.31.25_483_redacted.pdf page 1\n",
      "âš ï¸ OCR: lonza_guangzhou_guangdong_china_10.27-10.31.25_483_redacted.pdf page 2\n",
      "[11:35:53] âœ… PDF 34/56 | file=lonza_guangzhou_guangdong_china_10.27-10.31.25_483_redacted.pdf | chunks=5 | time=11.6s\n",
      "\n",
      "ðŸ“„ Processing lupin_limited_unit_3_pithampur_india_7.7-7.17.25_483_redacted.pdf\n",
      "[11:35:57] âœ… PDF 35/56 | file=lupin_limited_unit_3_pithampur_india_7.7-7.17.25_483_redacted.pdf | chunks=8 | time=4.1s\n",
      "\n",
      "ðŸ“„ Processing lupin_ltd._unit_2_pithampur_india_7.8-7.17.25_483_redacted.pdf\n",
      "[11:36:10] âœ… PDF 36/56 | file=lupin_ltd._unit_2_pithampur_india_7.8-7.17.25_483_redacted.pdf | chunks=24 | time=13.0s\n",
      "\n",
      "ðŸ“„ Processing molecular_pharmagroup_llc_fei_3014413265_fda_483_8-11-25_mb_redacted.508.pdf\n",
      "[11:36:19] âœ… PDF 37/56 | file=molecular_pharmagroup_llc_fei_3014413265_fda_483_8-11-25_mb_redacted.508.pdf | chunks=13 | time=8.6s\n",
      "\n",
      "ðŸ“„ Processing natco_pharma_telangana_india_6.9-6.19.25_483_redacted.pdf\n",
      "[11:36:34] âœ… PDF 38/56 | file=natco_pharma_telangana_india_6.9-6.19.25_483_redacted.pdf | chunks=28 | time=15.4s\n",
      "\n",
      "ðŸ“„ Processing nerpharma_srl._nerviano_italy_6-16_thru_27-25_483_redacted_2.pdf\n",
      "[11:36:49] âœ… PDF 39/56 | file=nerpharma_srl._nerviano_italy_6-16_thru_27-25_483_redacted_2.pdf | chunks=33 | time=15.3s\n",
      "\n",
      "ðŸ“„ Processing north_american_custom_laboratories_llc_dba_farmakeio_superior_custom_compounding_483_redacted.508.pdf\n",
      "[11:36:53] âœ… PDF 40/56 | file=north_american_custom_laboratories_llc_dba_farmakeio_superior_custom_compounding_483_redacted.508.pdf | chunks=6 | time=3.7s\n",
      "\n",
      "ðŸ“„ Processing pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 1\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 2\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 3\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 4\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 5\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 6\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 7\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 8\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 9\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 10\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 11\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 12\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 13\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 14\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 15\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 16\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 17\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 18\n",
      "âš ï¸ OCR: pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf page 19\n",
      "[11:38:31] âœ… PDF 41/56 | file=pharmathen_intl._rodopi_greece_11.10-11.21.25_483_redacted_0.pdf | chunks=46 | time=97.5s\n",
      "\n",
      "ðŸ“„ Processing pharmcore_inc._dba_hallandale_pharmacy_fei_3014480778_fda_483_6-13-25_mb_redacted.508.pdf\n",
      "[11:38:35] âœ… PDF 42/56 | file=pharmcore_inc._dba_hallandale_pharmacy_fei_3014480778_fda_483_6-13-25_mb_redacted.508.pdf | chunks=9 | time=4.9s\n",
      "\n",
      "ðŸ“„ Processing prorx_llc_issued_483_redacted.508.pdf\n",
      "[11:39:22] âœ… PDF 43/56 | file=prorx_llc_issued_483_redacted.508.pdf | chunks=31 | time=46.8s\n",
      "\n",
      "ðŸ“„ Processing pure_indulgence_southlake_tx_12.9.25_483.pdf\n",
      "[11:39:27] âœ… PDF 44/56 | file=pure_indulgence_southlake_tx_12.9.25_483.pdf | chunks=9 | time=4.9s\n",
      "\n",
      "ðŸ“„ Processing qualgen_llc_483_redacted.508.pdf\n",
      "[11:39:40] âœ… PDF 45/56 | file=qualgen_llc_483_redacted.508.pdf | chunks=28 | time=13.3s\n",
      "\n",
      "ðŸ“„ Processing rc_outsourcing_llc_final_483_redacted.508.pdf\n",
      "[11:39:51] âœ… PDF 46/56 | file=rc_outsourcing_llc_final_483_redacted.508.pdf | chunks=21 | time=10.5s\n",
      "\n",
      "ðŸ“„ Processing shilpa_medicare_jadcherla_telangana_india_11.10-11.21.25_483_redacted.pdf\n",
      "[11:40:28] âœ… PDF 47/56 | file=shilpa_medicare_jadcherla_telangana_india_11.10-11.21.25_483_redacted.pdf | chunks=71 | time=37.0s\n",
      "\n",
      "ðŸ“„ Processing sknv_ul_508.pdf\n",
      "[11:40:34] âœ… PDF 48/56 | file=sknv_ul_508.pdf | chunks=11 | time=6.3s\n",
      "\n",
      "ðŸ“„ Processing sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf\n",
      "âš ï¸ OCR: sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf page 1\n",
      "âš ï¸ OCR: sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf page 2\n",
      "[11:40:43] âœ… PDF 49/56 | file=sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf | chunks=6 | time=8.8s\n",
      "\n",
      "ðŸ“„ Processing sun_pharmaceuticals_medicare_halol_india_9.8-9.19.25_483_redacted.pdf\n",
      "âš ï¸ OCR: sun_pharmaceuticals_medicare_halol_india_9.8-9.19.25_483_redacted.pdf page 8\n",
      "[11:40:48] âœ… PDF 50/56 | file=sun_pharmaceuticals_medicare_halol_india_9.8-9.19.25_483_redacted.pdf | chunks=3 | time=4.8s\n",
      "\n",
      "ðŸ“„ Processing sun_pharma_gujarat_india_6.2-6.13.25_483_redacted.pdf\n",
      "[11:41:13] âœ… PDF 51/56 | file=sun_pharma_gujarat_india_6.2-6.13.25_483_redacted.pdf | chunks=53 | time=25.4s\n",
      "\n",
      "ðŸ“„ Processing ubi_pharma_inc_hsinchu_taiwan_10.6-10.14.25_483_redacted.pdf\n",
      "[11:41:26] âœ… PDF 52/56 | file=ubi_pharma_inc_hsinchu_taiwan_10.6-10.14.25_483_redacted.pdf | chunks=28 | time=13.0s\n",
      "\n",
      "ðŸ“„ Processing ul_675904.508.pdf\n",
      "[11:41:33] âœ… PDF 53/56 | file=ul_675904.508.pdf | chunks=12 | time=6.7s\n",
      "\n",
      "ðŸ“„ Processing wedgewood_connect_llc_300343972_fda483_07-03-2025_nb_redacted.508.pdf\n",
      "[11:41:40] âœ… PDF 54/56 | file=wedgewood_connect_llc_300343972_fda483_07-03-2025_nb_redacted.508.pdf | chunks=14 | time=7.0s\n",
      "\n",
      "ðŸ“„ Processing wellspharmacy_ul_508.pdf\n",
      "[11:41:48] âœ… PDF 55/56 | file=wellspharmacy_ul_508.pdf | chunks=14 | time=7.7s\n",
      "\n",
      "ðŸ“„ Processing zydus_lifescience_himachal_pradesh_india_8.13.25_483_redacted.pdf\n",
      "[11:41:57] âœ… PDF 56/56 | file=zydus_lifescience_himachal_pradesh_india_8.13.25_483_redacted.pdf | chunks=18 | time=9.3s\n",
      "âœ… Done uploading all PDFs\n",
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '188',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Thu, 29 Jan 2026 06:11:57 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '36',\n",
      "                                    'x-pinecone-request-id': '5029725094421152578',\n",
      "                                    'x-pinecone-request-latency-ms': '35',\n",
      "                                    'x-pinecone-response-duration-ms': '37'}},\n",
      " 'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 1355}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 1355,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Load pdf folder\n",
    "\n",
    "pdf_folder = r\"C:\\Users\\user\\FDA_RAG_BOT\\fda_483_docs\" # Path of your folder where you kept all your pdf\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "total = len(pdf_files)\n",
    "\n",
    "print(\"âœ… PDFs found:\", total)\n",
    "\n",
    "for i, f in enumerate(pdf_files, start=1):\n",
    "    start_time = time.time()\n",
    "    pdf_path = os.path.join(pdf_folder, f)\n",
    "\n",
    "    print(f\"\\nðŸ“„ Processing {f}\")\n",
    "    full_text = extract_full_text(pdf_path)\n",
    "    vectors = build_fda_chunks(full_text, source=f)\n",
    "\n",
    "    upsert_chunks_to_pinecone(vectors)\n",
    "\n",
    "    log_progress(i, total, f, len(vectors), start_time)\n",
    "\n",
    "print(\"âœ… Done uploading all PDFs\")\n",
    "print(index.describe_index_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda0d03f-ce67-4ff0-a45f-69f48973d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Firms loaded: 55\n"
     ]
    }
   ],
   "source": [
    "#Firm detection from simple name\n",
    "\n",
    "def load_firm_pdf_map(sample_k=5000):\n",
    "    dummy = get_embedding(\"firm index build\")\n",
    "    res = index.query(vector=dummy, top_k=sample_k, include_metadata=True)\n",
    "\n",
    "    firm_map = defaultdict(set)\n",
    "    for m in res[\"matches\"]:\n",
    "        md = m.get(\"metadata\", {})\n",
    "        firm = md.get(\"firm_name\")\n",
    "        src = md.get(\"source\")\n",
    "        if firm and src:\n",
    "            firm_map[firm].add(src)\n",
    "\n",
    "    return {k: sorted(list(v)) for k, v in firm_map.items()}\n",
    "\n",
    "FIRM_PDF_MAP = load_firm_pdf_map()\n",
    "print(\"âœ… Firms loaded:\", len(FIRM_PDF_MAP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be4e4dd9-b48a-4ceb-80d6-8da3189f9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firm detection from simple name\n",
    "\n",
    "def detect_firm_and_pdf(question: str):\n",
    "    q = question.lower()\n",
    "    q_tokens = set(re.findall(r\"[a-z0-9]+\", q))\n",
    "\n",
    "    best_firm = None\n",
    "    best_score = 0\n",
    "\n",
    "    for firm, pdfs in FIRM_PDF_MAP.items():\n",
    "        firm_tokens = set(firm.split())\n",
    "        score = len(q_tokens.intersection(firm_tokens))\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_firm = firm\n",
    "\n",
    "    # require at least 2 matching tokens for high precision\n",
    "    if best_firm and best_score >= 2:\n",
    "        # choose latest pdf (or first)\n",
    "        return best_firm, FIRM_PDF_MAP[best_firm][0]\n",
    "\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "833c9dad-fe70-423c-a622-5986b91030e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval\n",
    "\n",
    "def retrieve_context(question, pdf_name=None, top_k=12):\n",
    "    query_emb = get_embedding(question)\n",
    "\n",
    "    if pdf_name:\n",
    "        res = index.query(\n",
    "            vector=query_emb,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter={\"source\": {\"$eq\": pdf_name}}\n",
    "        )\n",
    "    else:\n",
    "        res = index.query(\n",
    "            vector=query_emb,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "    contexts = []\n",
    "    for m in res[\"matches\"]:\n",
    "        md = m.get(\"metadata\", {})\n",
    "        txt = md.get(\"text\", \"\")\n",
    "        src = md.get(\"source\", \"unknown\")\n",
    "        sec = md.get(\"section\", \"unknown\")\n",
    "        if txt.strip():\n",
    "            contexts.append(f\"(Source: {src}, Section: {sec})\\n{txt}\")\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa45044-8d0c-4b85-a3da-d553d8b08c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specially for observations retrieval\n",
    "\n",
    "def retrieve_all_observations(pdf_name, top_k=250):\n",
    "    query_emb = get_embedding(\"OBSERVATION 1 OBSERVATION 2 OBSERVATION 3\")\n",
    "\n",
    "    res = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"source\": {\"$eq\": pdf_name},\n",
    "            \"section_type\": {\"$eq\": \"observation\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    contexts = []\n",
    "    for m in res[\"matches\"]:\n",
    "        md = m.get(\"metadata\", {})\n",
    "        txt = md.get(\"text\", \"\")\n",
    "        sec = md.get(\"section\", \"\")\n",
    "        if txt.strip():\n",
    "            contexts.append(f\"(Source: {pdf_name}, {sec})\\n{txt}\")\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7983620-5873-44ee-b34f-233cf8a6031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Trimmer\n",
    "\n",
    "def trim_context(context: str, max_chars=160000):\n",
    "    return context[:max_chars] if len(context) > max_chars else context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded8a5b7-2674-40b7-be0b-4ed76678dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Calculation\n",
    "\n",
    "PRICES = {\"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60}}\n",
    "\n",
    "def calc_cost(model, input_tokens, output_tokens):\n",
    "    p = PRICES.get(model)\n",
    "    return (input_tokens/1_000_000)*p[\"input\"] + (output_tokens/1_000_000)*p[\"output\"]\n",
    "\n",
    "def chat_with_usage(system_prompt, user_prompt, model=\"gpt-4o-mini\"):\n",
    "    res = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    usage = res.usage\n",
    "    in_tok = usage.prompt_tokens\n",
    "    out_tok = usage.completion_tokens\n",
    "    total_tok = usage.total_tokens\n",
    "    cost = calc_cost(model, in_tok, out_tok)\n",
    "\n",
    "    return res.choices[0].message.content, in_tok, out_tok, total_tok, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e082931-9d4a-462c-9234-d85182c3d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a strict compliance QA assistant for FDA 483 documents.\n",
    "\n",
    "CRITICAL RULES (must follow):\n",
    "1) Use ONLY the provided CONTEXT from the PDFs. Do NOT use external knowledge, assumptions, or guesses.\n",
    "2) If the answer is not explicitly stated in the context, respond exactly: \"Not found in the provided PDFs.\"\n",
    "3) Do NOT paraphrase numbers, dates, names, or regulatory text. Copy them exactly as written.\n",
    "4) Be COMPLETE: if the question asks for observations/details, extract ALL relevant items from the context (do not miss any).\n",
    "5) Every claim must be supported by the context. If any claim is uncertain, mark it as \"Not found in the provided PDFs.\"\n",
    "6) Before finalizing, double-check that your answer is fully supported by the context and that no key point is missing.\n",
    "7) Always include citations in the form: (Source: <file>, Page: <page>).\n",
    "8) Check all the pages for every set of question, the answer should have all the deatils regarding to the question asked to the bot.\n",
    "9) Don't skip any part,i.e. if there are 10 observation in a pdf than you must include all the observations. \n",
    "I need all the observation/deatils if they are present in the pages of that particular pdf\n",
    "10) if question contains penalty/CAPA/NAI/VAI/OAI classification keywords and no match â†’ donâ€™t just say not found; give expert explanation.\n",
    "\n",
    "OUTPUT RULE:\n",
    "- Be precise and factual.\n",
    "- Prefer structured answers with bullet points and headings.\n",
    "- If the question is about observations, list them as Observation 1, Observation 2, etc. exactly as in the PDF text.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbb78d8e-f613-4be3-96a5-eb82e940d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect Observation Questions\n",
    "\n",
    "def is_observation_question(q: str):\n",
    "    q = q.lower()\n",
    "    return any(x in q for x in [\n",
    "        \"all observations\", \"list observations\", \"provide observations\",\n",
    "        \"what are the observations\", \"extract observations\",\"return all the observations\",\"all the subpoints A to Z\"\n",
    "    ])\n",
    "\n",
    "#RAG_Bot\n",
    "def rag_bot(question, top_k=12):\n",
    "    firm, pdf_name = detect_firm_and_pdf(question)\n",
    "\n",
    "    obs_mode = is_observation_question(question)\n",
    "\n",
    "    if obs_mode and pdf_name:\n",
    "        context = retrieve_all_observations(pdf_name, top_k=250)\n",
    "    else:\n",
    "        context = retrieve_context(question, pdf_name=pdf_name, top_k=top_k)\n",
    "\n",
    "    if not context.strip():\n",
    "        return \"Not found in the provided PDFs.\"\n",
    "\n",
    "    context = trim_context(context)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Answer with citations (Source + Section). No hallucination.\n",
    "\"\"\"\n",
    "\n",
    "    answer, in_tok, out_tok, total_tok, cost = chat_with_usage(\n",
    "        SYSTEM_PROMPT, prompt, model=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "    detected = f\"âœ… Detected firm: {firm} | PDF: {pdf_name}\" if pdf_name else \"âœ… No firm detected (global search)\"\n",
    "\n",
    "    return f\"\"\"{detected}\n",
    "\n",
    "âœ… Answer:\n",
    "{answer}\n",
    "\n",
    "--------------------------\n",
    "ðŸ“Š Usage:\n",
    "Input tokens: {in_tok}\n",
    "Output tokens: {out_tok}\n",
    "Total tokens: {total_tok}\n",
    "Estimated cost (USD): {cost:.6f}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dd7c365-3cc4-4e27-9f61-495eda685efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No firm detected (global search)\n",
      "\n",
      "âœ… Answer:\n",
      "ANSWER:\n",
      "\n",
      "### Observation 1\n",
      "- **Observation**: The number of containers to be sampled is not based upon appropriate criteria.\n",
      "  - Specifically, your firm samples [= container per lot of incoming raw materials and excipients, regardless of the total number of containers received. You have not established documented rationale or scientific justification to support this practice. The current sampling procedure does not provide assurance that samples are representative of the entire lot when multiple containers are received.  \n",
      "  (Source: sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf, Section: OBSERVATION 1)\n",
      "\n",
      "### Observation 2\n",
      "- **Observation**: Written documents are inadequate to ensure proper documentation and analysis of critical manufacturing operations.\n",
      "  - Specifically, Report VALRP-BTO-002395, Summary report for airflow visualization study and assessment of aseptic interventions during vial filling (Version 1.0, approved on September 3, 2025), failed to provide a comprehensive analysis of the smoke study videos and contained only a checklist of interventions performed.  \n",
      "  (Source: dr._reddys_telangana_india_9.4-9.12.25_483_redacted.pdf, Section: OBSERVATION 5)\n",
      "\n",
      "### Observation 3\n",
      "- **Observation**: There is a failure to thoroughly review any unexplained discrepancy whether or not the batch has been already distributed.\n",
      "  - Specifically, your procedure SOP-CAPS-4000582 \"Environmental Monitoring-503B\" allows for non-viable particulate (NVP) sample retesting without investigation following alert or action level excursions in ISO 5, 7, and 8 areas. Since January 2025, your firm has documented 8 instances of NVP excursion events where no investigations were initiated, instead relying on this retesting protocol.  \n",
      "  (Source: caps_allentown_300950582_483_090525_redacted.508.pdf, Section: OBSERVATION 5)\n",
      "\n",
      "### Observation 4\n",
      "- **Observation**: There is a failure to thoroughly review any unexplained discrepancy and the failure of a batch or any of its components to meet any of its specifications whether or not the batch has been already distributed.\n",
      "  - Specifically, you manufactured _______mJect10n via as drug product. You received at least 15 market complaints for partially filled vials, empty vials, and crack/leak vial. You investigated only 3 (three) of these market complaints.  \n",
      "  (Source: ubi_pharma_inc_hsinchu_taiwan_10.6-10.14.25_483_redacted.pdf, Section: OBSERVATION 5)\n",
      "\n",
      "### Observation 5\n",
      "- **Observation**: The accuracy, sensitivity, specificity and reproducibility of test methods have not been established.\n",
      "  - Specifically, the analytical method for (bY{4 Tablets USP has not been evaluated for accuracy, sensitivity, specificity and reproducibility according to the Assistant General Manager of QA.  \n",
      "  (Source: cohance_lifesciences_ltd._telangana_india_8.4-8.12.25_483_redacted.pdf, Section: OBSERVATION 6)\n",
      "\n",
      "### Observation 6\n",
      "- **Observation**: Your firm's documentation practices are deficient.\n",
      "  - Specifically, during batch record review, an unexecuted manufacturing step was signed as having been performed by the operator and verified by a second person as having been observed. A lack of good documentation practices is also evidenced by at least thirty-five deviations related to incomplete documentation and missing second person verification.  \n",
      "  (Source: alvotech_hf_rekjavik_iceland_6.26-7.4.25_483_redacted.pdf, Section: OBSERVATION 7)\n",
      "\n",
      "### Observation 7\n",
      "- **Observation**: There is a failure to establish adequate procedures for the control of the manufacturing process.\n",
      "  - Specifically, your firm has not established adequate procedures for the control of the manufacturing process, which has resulted in the production of out-of-specification products.  \n",
      "  (Source: dr._reddys_laboratories_limited_unit_6_srikakulam_india_6.7.24_483.pdf, Section: OBSERVATION 1)\n",
      "\n",
      "### Observation 8\n",
      "- **Observation**: The firm has not established adequate procedures for the control of the manufacturing process.\n",
      "  - Specifically, your firm has not established adequate procedures for the control of the manufacturing process, which has resulted in the production of out-of-specification products.  \n",
      "  (Source: cohance_lifesciences_ltd._telangana_india_8.4-8.12.25_483_redacted.pdf, Section: OBSERVATION 1)\n",
      "\n",
      "### Observation 9\n",
      "- **Observation**: The firm has not adequately investigated deviations.\n",
      "  - Specifically, your firm has not adequately investigated deviations related to the manufacturing process, which has resulted in the production of out-of-specification products.  \n",
      "  (Source: dr._reddys_telangana_india_9.4-9.12.25_483_redacted.pdf, Section: OBSERVATION 2)\n",
      "\n",
      "### Observation 10\n",
      "- **Observation**: The firm has not established adequate procedures for the control of the manufacturing process.\n",
      "  - Specifically, your firm has not established adequate procedures for the control of the manufacturing process, which has resulted in the production of out-of-specification products.  \n",
      "  (Source: sumitomo_pharma_co._ltd._suzuka_japan_8.3-8.7.25_483_redacted.pdf, Section: OBSERVATION 1)\n",
      "\n",
      "--------------------------\n",
      "ðŸ“Š Usage:\n",
      "Input tokens: 5036\n",
      "Output tokens: 1163\n",
      "Total tokens: 6199\n",
      "Estimated cost (USD): 0.001453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(rag_bot(\"List all observations with all the subpoints in cohance lifesciences. Return all observation fully with all the sub groups in Cohance lifescience. Additionally provide me what steps as a new\"))\n",
    "#print(rag_bot(\"List all observations with all the subpoints(A,B,C,D,E,F) in cohance lifesciences. Return all observation fully with all the sub groups in Cohance lifescience. additionally, Study these observations and tell me what things we should avoide to have a successful fda quality check. \"))#print(rag_bot(\"Is there any reference to CAPA effectiveness checks? Quote the lines. Scan for the pdf for this word 'CAPA' and provide me all the deatils with CAPA word in all the pdfs which you have in the system from provide the source.\"))\n",
    "print(rag_bot(\"Return Some 10 common observations fully from all the pdfs which you have with all the subpoints without summarizing\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0cb4b4-2bd6-4220-86f0-4596f8669d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
